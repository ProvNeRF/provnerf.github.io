<!DOCTYPE html>
<html lang="en">
  <head>
    <br>
    <!-- Basic Page Needs
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <meta charset="utf-8">
    <title>ProvNeRF</title>
    <meta name="description" content="">
    <meta name="author" content="">

    <!-- Mobile Specific Metas
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- FONT
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <link href="https://fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

    <!-- CSS
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <link rel="stylesheet" href="css/normalize.css">
    <link rel="stylesheet" href="css/skeleton.css">
    <link rel="stylesheet" href="css/footable.standalone.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" rel="stylesheet">
  
    <!-- Favicon
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <link rel="icon" type="image/png" href="images/favicon.png">

    <!-- Google icon -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">

    <!-- Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                               m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
                              })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-86869673-1', 'auto');
      ga('send', 'pageview');
    </script>

    <!-- Slider -->
    <link rel="stylesheet" href="css/dics.css">
    <script src="js/dics.js"></script>

    <!-- Hover effect: https://codepen.io/nxworld/pen/ZYNOBZ -->
    <style>

    .uniform-height {
        height: 315px; /* Set the height for all images */
        width: auto; /* Preserve the aspect ratio */
        object-fit: cover; /* Fill the specified height while keeping the aspect ratio */
        display: block; /* Avoid inline spacing issues */
    }

    img {
      margin: 0;
      padding: 0;
    }

      .column-50 {
          float: left;
          width: 50%;
      }
      .row-50:after {
          content: "";
          display: table;
          clear: both;
      }

      .floating-teaser {
          float: left;
          width: 30%;
          text-align: center;
          padding: 15px;
      }
      .venue strong {
          color: #99324b;
      }



      .benchmark {
          width: 100%;
          max-width: 960px;
          overflow: scroll;
          overflow-y: hidden;
      }
      
      @media (min-width: 550px) {
      .column.has-text-centered {
        margin-left: 0 !important;
      }
    }

    .publication-links {
      display: flex;
      justify-content: center;
      flex-wrap: wrap;
      gap: 10px;
      margin-bottom: 30px;
    }

    .link-block {
      display: inline-block;
    }

    .external-link {
      display: flex;
      align-items: center;
      background-color: #363636;
      color: #fff;
      text-decoration: none;
      padding: 8px 12px;
      border-radius: 24px;
      font-size: 14px;
      font-weight: normal;
      text-transform: none;
    }

    iframe {
    width: 560px;
    height: 315px;
    margin: 0 auto;
    display:block;
    background-color: #777;
}

    .external-link .icon {
      margin-right: 6px;
    }

    .external-link .icon i {
      font-size: 16px;
    }

    .external-link:hover {
      background-color: #4a4a4a;
    }
    </style>
  </head>
  <body>

    <!-- Primary Page Layout
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="container">
      <h4 style="text-align:center">ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Field</h4>
      <p align="center", style="margin-bottom:12px;">
        <a class="simple" href="https://georgenakayama.github.io/">Kiyohiro Nakayama</a><sup>1</sup>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        <a class="simple" href="https://mikacuy.github.io/">Mikaela Angelina Uy</a><sup>1, 3</sup>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        <a class="simple" href="https://qq456cvb.github.io/">Yang You</a><sup>1</sup>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        <a class="simple" href="http://www.sfu.ca/~keli/">Ke Li</a><sup>2</sup>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        <a class="simple" href="https://geometry.stanford.edu/member/guibas/">Leonidas J. Guibas</a><sup>1</sup>
      </p>

      <p align="center" style="margin-bottom:20px;">
        <sup>1</sup>Stanford University
        <span style="display:inline-block; width: 32px"></span>
        <sup>2</sup>Simon Fraser University
        <span style="display:inline-block; width: 32px"></span>
        <sup>3</sup>NVIDIA<br>
      </p>

      <!-- Additional Links Section -->
    <div class="column has-text-centered">
      <div class="publication-links">
        <!-- arXiv Link -->
        <span class="link-block">
          <a href="https://arxiv.org/abs/2401.08140" class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="ai ai-arxiv"></i>
            </span>
            <span>arXiv</span>
          </a>
        </span>
        <!-- Code Link -->
        <span class="link-block">
          <a href="https://github.com/ProvNeRF/ProvNeRF" class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="fab fa-github"></i>
            </span>
            <span>Code</span>
          </a>
        </span>

      </div>
    </div>

      <!-- <div class="venue">
        <p align="center"> International Conference on Computer Vision (ICCV), 2023 </p>
      </div> -->

      <figure>
        <img src="images/teaser.jpg" style="width:100%"></img>
        <!-- <br> -->
      </figure>
      <div class="caption">
        <b>(Left)</b> ProvNeRF models a provenance field that outputs <em>provenances</em> for each 3D point as likely samples (arrows). 
        For 3D points (brown triangle and blue circle), the corresponding provenances (illustrated by the arrows), are locations that likely observe them. 
        <b>(Right)</b> ProvNeRF enables better novel view synthesis and estimating the uncertainty of the capturing process 
        because it models the locations of likely observations that is critical for NeRF's optimization.
        <br>
          <br>
      </div>
      

      <div id="teaser" class="container" style="width:100%; margin:0; padding:0">
        
        <h5>Abstract</h5>
        <p align="justify">
          Neural radiance fields (NeRFs) have gained popularity with multiple works showing promising results across various applications. 
          However, to the best of our knowledge, existing works do not explicitly model the distribution of training camera poses, 
          or consequently the triangulation quality, a key factor affecting reconstruction quality dating back to classical vision literature.
          We close this gap with ProvNeRF, an approach that models the <b>provenance for each point</b> -- i.e., the locations where it is likely visible -- 
          of NeRFs as a stochastic field. We achieve this by extending implicit maximum likelihood estimation (IMLE) to functional space with an optimizable objective. 
          We show that modeling per-point provenance during the NeRF optimization enriches the model with information 
          on triangulation leading to improvements in novel view synthesis and uncertainty estimation under the challenging sparse, 
          unconstrained view setting against competitive baselines. 
          <br>
          <br>
        </p>
      </div>
      <br><br>
      <h5>Method Overview</h5>
      <figure>
        <img src="images/pipeline.jpg" style="width:100%"></img>
        <!-- <br> -->
      </figure>
      <div class="caption">
        <b>Training pipeline for ProvNeRF</b> For each point <b>x</b> seen from provenance, 
        with direction d at distance t, we first sample K latent random functions from distribution. 
        The learned transformation <b>H</b> transforms each latent function sample to a provenance sample. 
        <br>
          <br>
      </div>
      <h5>Supplementary Video</h5>
      <iframe width="560" height="315" src="https://www.youtube.com/embed/Z5A01Dywi24?si=XR63DbKouxCK2Qx9" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>


      <br><br>
      <h5>Application 1: <b>Uncertainty Modeling</b></h5>
      <figure>
        <img src="images/triang.png" style="width:100%">
      </figure>
      <div class="caption">
        Uncertainty of the capturing process strongly depends on the angles between observations. 
        For example, a larger camera baseline would lead to small regions of uncertainty. 
        a small baseline leads to large uncertainty region. 
        We leverage literature on multi-view geometry to quantify this intuition as the posterior probability of each point given the provenance predictions.  
      </div>
      <br><br>
      <h6><b>Scannet 710</b></h6>
      <div class="b-dics">
          <img src="images/710_unc/provnerf.png" alt="Ours">
          <img src="images/710_unc/bayes.png" alt="Bayes' Rays">
      </div>
      <figure>
          <img src="images/710_unc/all.png" style="width:100%">
      </figure>

      <h6><b>Scannet 758</b></h6>
      <div class="b-dics">
          <img src="images/758_unc/provnerf.png" alt="Ours">
          <img src="images/758_unc/bayes.png" alt="Bayes' Rays">
      </div>
      <figure>
          <img src="images/758_unc/all.png" style="width:100%">
      </figure>
      <div class="caption">
        <b>Uncertainty Modeling</b>. The uncertainty and depth error maps are shown
        with color bars specified. Uncertainty values and depth errors are normalized per test image for the result to be
        comparable.
      </div>
      <br><br>
      <h5>Application 2: <b>Novel View Synthesis</b></h5>
      <!-- <video width="100%" autoplay muted controls>
        <source src="videos/reg.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video> -->
      <figure>
        <img src="/Users/georgenakayama/Desktop/banner" style="width:100%">
    </figure>
      <div class="caption">
        ProvNeRF also helps with improving novel view synthesis by removing additional floaters. 
        Given a pretrained NeRF with floaters, we can query ProvNeRF on points along a training ray to obtain their provenance samples. 
        By matching the transmittance rendered from the provenance rays with that from the training ray, floaters in between can be removed. We formulate this as an additional regularizer on top of NeRF training. 

      </div>
      <br><br>
      <div class="b-dics">
          <img class="uniform-height" src="images/reg/ours.png" alt="Ours">
          <img class="uniform-height" src="images/reg/scade.png" alt="SCADE">
      </div>
      <div class="caption">
        Notice we are able to remove clouds and small floaters from the original SCADE model.
      </div>
      <!-- <h5>Application 2: <b>Criteria-based Viewpoint Optimization</b></h5> -->
      <!-- <figure> -->
        <!-- <img src="images/close_up/all.png" style="width:100%"></img> -->
        <!-- <br> -->
      <!-- </figure> -->
      <!-- <div class="caption"> -->
        <!-- <b>Close-up View Optimization</b> We show the optimized -->
        <!-- view comparison of our provenance-aided viewpoint selection compared to the baselines under the close-up objective.  -->
        <!-- Notice that our method (in red) both maximizes the plush's area size while obtaining a high quality reconstruction. On the other hand, both the retrieval and optimization baselines fail to balance between the -->
        <!-- two. -->
      <!-- </div> -->
      <!-- <figure> -->
        <!-- <img src="images/close_up/graph.png" style="width:100%"></img> -->
        <!-- <br> -->
      <!-- </figure> -->
      <!-- <div class="caption"> -->
        <!-- <b>Close-up View Optimization</b> Graph showning PSNR and Area Size plots with the objective of maximizing the projected area of the target. Notice that our provenance-aided optimization obtains the best balance between area size and PSNR. -->
      <!-- </div> -->
      <!-- <br><br> -->
      <!-- <figure> -->
        <!-- <img src="images/normal/all.png" style="width:100%"></img> -->
        <!-- <br> -->
      <!-- </figure> -->
      <!-- <div class="caption"> -->
        <!-- <b>Normal Vector Alignment View Optimization</b> We show the optimized -->
        <!-- view comparison of our provenance-aided viewpoint selection compared to the baselines under the normal vector view alignment objective.  -->
        <!-- Notice that our method is able to obtain a bird-eye view of the book while keeping the book within its viewpoint. On the other hand, the retrieved view fails to align with the normal of the book while the optimized view simply does not see the book at all. -->
      <!-- </div> -->
      <!-- <figure> -->
        <!-- <img src="images/normal/graph.png" style="width:100%"></img> -->
        <!-- <br> -->
      <!-- </figure> -->
      <!-- <div class="caption"> -->
        <!-- <b>Normal Vector Alignment View Optimization</b> Graph showing PSNR and Dot product with the normal vector with the objective of normal vector alignment. Notice that our provenance-aided optimization obtains the best balance between dot product and PSNR. -->
      <!-- </div> -->
      <!-- <br><br> -->
      <!-- <figure> -->
        <!-- <img src="videos/part_mixing.gif" style="width:100%"></img> -->
        <!-- <br> -->
      <!-- </figure> -->
      <!-- <div class="caption"> -->
        <!-- <b>DiffFacto Application:</b> Part mixing. -->
      <!-- </div> -->
      <!-- <br><br> -->
      

      <!-- <div id="teaser" class="container" style="width:100%; margin:0; padding:0"> -->
        <!-- <h5>Video</h5> -->
        <!-- <p align="center"><iframe width="75%" height="450" -->
        <!-- src="https://www.youtube.com/embed/gwlqiJP5izI"> -->
        <!-- </iframe></p> -->
      <!-- </div> -->

  <!-- <div id="teaser" class="container" style="width:100%; margin:0; padding:0"> -->
    
    <!-- <h5>Qualitative Results</h5> -->
    <!-- <center> -->
      <!-- <img src="images/unc_oner.jpg" style="width:90%"></img> -->
        <!-- <div class="caption" > -->
          <!-- <p align="justify"> -->
            <!-- <b>Generated Shapes with Controlled Variation:</b>  -->
            <!-- <u>Left:</u> Re-sampling a selected part style while keeping the rest fixed.  -->
            <!-- <u>Middle:</u> Fixing a selected part while re-sampling the rest.  -->
            <!-- <u>Right:</u> Generating multiple part configurations for a given set of part styles. Gray refers to the fixed part while colored parts are being modified. -->
          <!-- </p> -->
        <!-- </div> -->
        <!-- <br> -->

      <!-- <img src="images/main2.jpg" style="width:90%"></img> -->
        <!-- <div class="caption" > -->
          <!-- <p align="justify"> -->
            <!-- <b>Applications:</b>    -->
            <!-- <u>Left:</u> Part Style Mixing. The colored parts from the left are selected and mixed to provide the shapes on the right. -->
            <!-- <u>Right:</u> Part Interpolation. We interpolate the chair backs (red) in the first 2 rows and the lamp poles (orange) in the last 2 rows (indicated by the hand icons). -->
          <!-- </p> -->
        <!-- </div> -->
        <!-- <br> -->

      <!-- <img src="images/diff_step.jpg" style="width:95%"></img> -->
        <!-- <div class="caption" > -->
          <!-- <p align="justify"> -->
            <!-- <u>Left:</u> Visualization of the forward diffusion process with our generalized forward kernel. Compared to the forward diffusion of the vanilla diffusion (bottom row, taken from <a href="https://alexzhou907.github.io/pvd">PVD</a>) that drifts all points uniformly towards the unit Gaussian, our generalized forward kernel diffuses points towards different Gaussains based on the part configurations.  -->
            <!-- <u>Right:</u> Examples of lamp reconstruction with our generalized forward kernel <b>(With Fwk.)</b> versus without the generalized kernel formulation <b>(Without Fwk.)</b>. Heat maps showing relative errors in the reconstruction lamps. Red indicates larger differences.  -->
          <!-- </p> -->
        <!-- </div> -->
        <!-- <br> -->

        

    <!-- </center> -->
  <!-- </div> -->

		<!-- -->

  <br> <br>
	<div class="section">
    <h5>Citation</h5> 
    <pre style="margin:0"><code>@inproceedings{nakayama2024provnerf,
        title={ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Field}, 
        author={Kiyohiro Nakayama and Mikaela Angelina Uy and Yang You and Ke Li and Leonidas Guibas},
        journal = {Advances in Neural Information Processing Systems (NeurIPS)}},
        year={2024}
    }</code></pre>    			
	</div>
		

  <br> <br>
  <div class="section">
      <h5>Acknowledgements</h5>            
      <p>
        This work is supported by ARL grant W911NF-21-2-0104, a Vannevar Bush Faculty Fellowship, an Apple Scholars in AI/ML PhD Fellowship, a Snap Research Fellowship, the Outstanding Doctoral Graduates Development Scholarship of Shanghai Jiao Tong University, and the Natural Sciences and Engineering Research Council of Canada.
			</p>
  </div>
  <!-- </div> -->

    <script type="text/javascript" src="../js/jquery.min.js"></script>
    <script type="text/javascript" src="../js/footable.min.js"></script>

    <script type="text/javascript">
      jQuery(function($){
          $('.table').footable();
      });
    </script>
    <script>

    document.addEventListener('DOMContentLoaded', domReady);

    function domReady() {
        const bDicsElements = document.querySelectorAll('.b-dics');

        bDicsElements.forEach((bDicsContainer) => {
            new Dics({
                container: bDicsContainer, // Pass each element directly
                textPosition: 'right',
                linesOrientation: "vertical",
                arrayBackgroundColorText: ["#FFFFFF", "#000000"],
                arrayColorText: ["#000000", "#FFFFFF"],
                linesColor: "rgb(0,0,0)"
            });
        });
    }
  </script>

    <!-- End Document
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  </body>
</html>
